{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cfa3d1-2187-4176-8c23-19bdf0925364",
   "metadata": {},
   "source": [
    "# Korean LLM (Large Language Model) fine-tuning on Local environment (Debugging)\n",
    "---\n",
    "\n",
    "- 허깅페이스 인증 정보 설정: `huggingface-cli login`\n",
    "    - https://huggingface.co/join\n",
    "    - https://huggingface.co/settings/tokens\n",
    "    \n",
    "\n",
    "## Overview \n",
    "\n",
    "본격적으로 SageMaker 훈련 인스턴스로 훈련을 수행하기 전에 SageMaker Notebook / SageMaker Studio / SageMaker Studio Lab 에서 샘플 데이터로 디버깅을 수행합니다.\n",
    "물론 온프레미스 환경에서 디버깅을 수행할 수 있다면, 기존 환경과 동일하게 디버깅을 수행하면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e8020b-9964-4198-aed5-0efca878bf5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-30 00:24:44,344] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n",
    "\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b7bd04-d647-45cc-a340-84505df99f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r bucket_prefix dataset_prefix s3_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2745978-138f-4b70-8693-64859f4704ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset_prefix\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] 1번 모듈 노트북을 다시 실행해 주세요.\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44b3088-f0b9-4897-a556-933475a9880a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_dataset = load_from_disk(dataset_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f77db-7a07-4439-98c0-2e19cc72e0cb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Load Model\n",
    "---\n",
    "한정된 GPU 메모리 안에 LLM 모델을 로드하는 것은 매우 어렵습니다. 예컨대 20B의 모델을 로드하려면 fp32 기준으로 80GB 이상의 메모리가 필요하고 fp16 기준으로도 40GB 이상의 GPU 메모리가 필요하며, 파인 튜닝을 수행하는 경우는 이보다 더욱 많은 GPU 메모리가 필요합니다. 이런 경우 4비트 양자화와 LoRA를 사용하면 범용적으로 사용하고 있는 16GB 및 24GB GPU 메모리로도 파인 튜닝이 가능합니다. 현 기준으로는 4비트 양자화를 지원하는 QLoRA 기법이 가장 널리 사용되고 있으며 bitsandbytes를 사용하여 QLoRA를 쉽게 적용할 수 있습니다. QLoRA는 양자화된 파라미터의 분포 범위를 정규 분포 내로 억제하여 정밀도의 저하를 방지하는 4비트 NormalFloat 양자화 양자화를 적용하는 정수에 대해서도 양자화를 적용하는 이중 양자화, 그리고 optimizer state 등의 데이터를 CPU 메모리에 저장하는 페이징 기법을 적용하여 GPU 메모리 사용량을 억제합니다. QLoRA에 대한 자세한 내용은 논문 (https://arxiv.org/pdf/2305.14314.pdf) 을 참조하기 바랍니다.\n",
    "\n",
    "### Create a bitsandbytes configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965584cc-3956-4711-8eb4-4fbdaa41f397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "quant_4bit = True\n",
    "quant_8bit = False\n",
    "\n",
    "if quant_4bit:\n",
    "    nf4_config = BitsAndBytesConfig(\n",
    "       load_in_4bit=True,\n",
    "       bnb_4bit_quant_type=\"nf4\",\n",
    "       bnb_4bit_use_double_quant=True,\n",
    "       bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "else:\n",
    "    nf4_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da734ff-5488-4ead-9004-9b5a049b0070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID = \"nlpai-lab/kullm-polyglot-12.8b-v2\"\n",
    "\n",
    "# create model dir\n",
    "model_name = HF_MODEL_ID.split(\"/\")[-1].replace('.', '-')\n",
    "model_tar_dir = Path(f\"/home/ec2-user/SageMaker/models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b565d1a-5969-4bd3-8233-cce8ad1e245e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c820318c294b0c93ea9fd4a44ff36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device_map = \"auto\"\n",
    "\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(HF_MODEL_ID)\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    model_tar_dir,\n",
    "    load_in_8bit=True if quant_8bit else False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    "    #cache_dir=cache_dir,\n",
    "    quantization_config=nf4_config, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b37ebe-c475-47ee-993f-464c0d7201d0",
   "metadata": {},
   "source": [
    "### Create LoRA config\n",
    "LoRA 설정에 대한 자세한 내용은 아래를 참조해 주세요.\n",
    "- https://huggingface.co/docs/peft/conceptual_guides/lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d0fcf8-67a0-44cc-9b89-c6f26a8dcc2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_r  = 8\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\"query_key_value\", \"xxx\"]\n",
    "    \n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35312b9f-98d5-410b-b120-4156c5cce185",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,553,600 || all params: 6,608,701,440 || trainable%: 0.09916622894073424\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488f765-058c-434b-8ed1-b64a490cf96c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Training\n",
    "---\n",
    "### Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "415f898f-760f-4ffa-ba5c-3e93c0dee47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = lm_dataset#.select(range(20))\n",
    "val_data = None\n",
    "num_epochs = 1\n",
    "batch_size = 2\n",
    "\n",
    "learning_rate = 3e-4\n",
    "gradient_accumulation_steps = 2\n",
    "val_set_size = 0\n",
    "output_dir = 'output'\n",
    "world_size = 1\n",
    "ddp = world_size != 1\n",
    "group_by_length = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55083ad0-a671-4634-9912-e036b18f29d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    logging_steps=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200 if val_set_size > 0 else None,\n",
    "    output_dir=output_dir,\n",
    "    load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "    ddp_find_unused_parameters=False if ddp else None,\n",
    "    group_by_length=group_by_length,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=train_args,\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66470982-1dad-4e3d-9e29-a736fab51cf0",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a70473b-78dd-4ede-9b81-4980e9404c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 06:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.968800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.885900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.861500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.846400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.851600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.820800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.862300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.775500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.742300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.743600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "    model, type(model)\n",
    ")\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d0a1a-d4f6-4650-a2f2-af5375a126a5",
   "metadata": {},
   "source": [
    "### Check metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e72538-ca9e-4fe0-9927-61c4e24d6f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "#trainer.save_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8155ff-b6b8-493a-96bf-321f605f9fad",
   "metadata": {},
   "source": [
    "### Save fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e04694-47c4-45b1-a034-09812e21dc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93c053-0543-4da7-9e6a-7b0f95e160d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Free memory for merging weights\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
