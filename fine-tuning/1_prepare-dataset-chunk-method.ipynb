{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8197b9-f604-42ce-90d4-d02cafa91aa3",
   "metadata": {},
   "source": [
    "# Preprocess Datasets for Korean LLM (Large Language Model) fine-tuning\n",
    "---\n",
    "\n",
    "- AWS 기술 블로그에서 전처리했던 방식대로 전처리 수행\n",
    "    - https://aws.amazon.com/ko/blogs/tech/train-a-large-language-model-on-a-single-amazon-sagemaker-gpu-with-hugging-face-and-lora/\n",
    "- 허깅페이스 인증 정보 설정: `huggingface-cli login`\n",
    "    - https://huggingface.co/join\n",
    "    - https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "283f4cee-20d4-4b94-930b-da48f05d03db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../templates')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dea617-b3f6-4196-a5d0-b7a39e0691b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Download LLM from Hugging Face hub\n",
    "---\n",
    "\n",
    "### Load dataset\n",
    "허깅페이스 허브에서 다운로드하거나 json/json 포맷의 데이터 세트를 다운로드합니다. 데이터 세트 내 샘플은 (`instruction, input, output`)의 key-value나 (`instruction, output`)의 key-value로 구성되어야 합니다.\n",
    "\n",
    "예시:\n",
    "```\n",
    "{\n",
    "    \"instruction\":\"건강을 유지하기 위한 세 가지 팁을 알려주세요.\",\n",
    "    \"input\":\"\",\n",
    "    \"output\":\"세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57666867-edaa-4b16-8e7a-d2af661e3c42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ec2-user/.cache/huggingface/datasets/beomi___parquet/beomi--KoAlpaca-v1.1a-1465f66eb846fd61/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cba9149b2bd4cb9acc5590f490b74e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from inference_lib import Prompter\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n",
    "\n",
    "\n",
    "data_path = \"nlpai-lab/kullm-v2\"\n",
    "data_path = \"beomi/KoAlpaca-v1.1a\"\n",
    "#data_path = \"./data/ko_alpaca_data.json\"\n",
    "\n",
    "if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "    data = load_dataset(\"json\", data_files=data_path)\n",
    "else:\n",
    "    data = load_dataset(data_path)\n",
    "    \n",
    "prompter = Prompter(\"kullm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26d9d237-1fd6-49da-a177-f96012366162",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID = \"nlpai-lab/kullm-polyglot-12.8b-v2\"\n",
    "\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(HF_MODEL_ID)\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\"]\n",
    "\n",
    "# create model dir\n",
    "model_name = HF_MODEL_ID.split(\"/\")[-1].replace('.', '-')\n",
    "model_tar_dir = Path(f\"/home/ec2-user/SageMaker/models/{model_name}\")\n",
    "if not os.path.isdir(model_tar_dir):\n",
    "    model_tar_dir.mkdir(exist_ok=True)\n",
    "    # Download model from Hugging Face into model_dir\n",
    "    snapshot_download(\n",
    "        HF_MODEL_ID, \n",
    "        local_dir=str(model_tar_dir), \n",
    "        local_dir_use_symlinks=False,\n",
    "        allow_patterns=allow_patterns,\n",
    "        cache_dir=\"/home/ec2-user/SageMaker/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68e8b5-2bc1-47c3-9f6f-4db9f3ddd6f8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Tokenize\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a492cfc-8402-4fb1-8fe3-3b27c64c5c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point.get(\"input\"),\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    data_point['text'] = full_prompt\n",
    "    return data_point\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b870f0e1-eab7-45db-bd59-3bd5c9c19394",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 3586\n"
     ]
    }
   ],
   "source": [
    "dataset = data['train'].shuffle()#.select(range(100))\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=1536),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc49f3f1-bca8-4472-b491-40849970ed90",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Save dataset to S3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1328ad7-c912-4da5-a905-2859e2276cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker role arn: arn:aws:iam::143656149352:role/service-role/AmazonSageMaker-ExecutionRole-20220317T150353\n",
      "SageMaker bucket: sagemaker-us-east-1-143656149352\n",
      "SageMaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "bucket = None\n",
    "if bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket)\n",
    "\n",
    "print(f\"SageMaker role arn: {role}\")\n",
    "print(f\"SageMaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"SageMaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d4986c9-05bc-4c10-94df-265863dfb316",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 data path: \n",
      " s3://sagemaker-us-east-1-143656149352/ko-llms/peft/kullm-polyglot-12-8b-v2/dataset/chunk-train\n",
      "S3 pretrained model path: \n",
      " s3://sagemaker-us-east-1-143656149352/ko-llms/peft/huggingface-models/kullm-polyglot-12-8b-v2/\n"
     ]
    }
   ],
   "source": [
    "bucket_prefix = 'ko-llms/peft'\n",
    "dataset_prefix = 'chunk-train'\n",
    "s3_data_path = f\"s3://{bucket}/{bucket_prefix}/{model_name}/dataset/{dataset_prefix}\"\n",
    "s3_pretrained_model_path = f\"s3://{bucket}/{bucket_prefix}/huggingface-models/{model_name}/\"\n",
    "print(f\"S3 data path: \\n {s3_data_path}\")\n",
    "print(f\"S3 pretrained model path: \\n {s3_pretrained_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "205c6402-39b0-47f3-b8dd-6f2ef3add0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3586 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for debugging: 50\n"
     ]
    }
   ],
   "source": [
    "num_debug_samples = 50\n",
    "lm_dataset.save_to_disk(s3_data_path)\n",
    "lm_dataset.select(range(num_debug_samples)).save_to_disk(dataset_prefix)\n",
    "print(f\"Number of samples for debugging: {num_debug_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d36dd79-6b74-475c-bbeb-bc56d1564580",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'bucket_prefix' (str)\n",
      "Stored 'dataset_prefix' (str)\n",
      "Stored 's3_data_path' (str)\n"
     ]
    }
   ],
   "source": [
    "%store bucket_prefix dataset_prefix s3_data_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
