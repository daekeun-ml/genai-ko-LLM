{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e1e3ce5-d856-4cf8-93b7-50e0acce9f15",
   "metadata": {},
   "source": [
    "# Preprocess Datasets for Korean LLM (Large Language Model) fine-tuning\n",
    "---\n",
    "\n",
    "- Alpaca 논문에서 전처리했던 방식대로 전처리 수행\n",
    "- 허깅페이스 인증 정보 설정: `huggingface-cli login`\n",
    "    - https://huggingface.co/join\n",
    "    - https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f4cee-20d4-4b94-930b-da48f05d03db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../templates')\n",
    "\n",
    "from common_lib import check_packages\n",
    "check_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088d0bd-fd82-4dc4-a9e4-572b96df59d5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Download LLM from Hugging Face hub\n",
    "---\n",
    "\n",
    "### Load dataset\n",
    "허깅페이스 허브에서 다운로드하거나 json/json 포맷의 데이터 세트를 다운로드합니다. 데이터 세트 내 샘플은 (`instruction, input, output`)의 key-value나 (`instruction, output`)의 key-value로 구성되어야 합니다.\n",
    "\n",
    "예시:\n",
    "```\n",
    "{\n",
    "    \"instruction\":\"건강을 유지하기 위한 세 가지 팁을 알려주세요.\",\n",
    "    \"input\":\"\",\n",
    "    \"output\":\"세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f226ac4-c157-4ee3-abd7-b9519320e229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from inference_lib import Prompter\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n",
    "\n",
    "data_path = \"nlpai-lab/kullm-v2\"\n",
    "#data_path = \"beomi/KoAlpaca-v1.1a\"\n",
    "#data_path = \"./data/ko_alpaca_data.json\"\n",
    "\n",
    "if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "    data = load_dataset(\"json\", data_files=data_path)\n",
    "else:\n",
    "    data = load_dataset(data_path)\n",
    "    \n",
    "prompter = Prompter(\"kullm\")\n",
    "cutoff_len = 2048\n",
    "train_on_inputs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94014370-6f4b-4243-8c98-534cfc5eca36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID = \"nlpai-lab/kullm-polyglot-12.8b-v2\"\n",
    "\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(HF_MODEL_ID)\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\"]\n",
    "\n",
    "# create model dir\n",
    "model_name = HF_MODEL_ID.split(\"/\")[-1].replace('.', '-')\n",
    "model_tar_dir = Path(f\"/home/ec2-user/SageMaker/models/{model_name}\")\n",
    "if not os.path.isdir(model_tar_dir):\n",
    "    os.makedirs(model_tar_dir, exist_ok=True)\n",
    "    # Download model from Hugging Face into model_dir\n",
    "    snapshot_download(\n",
    "        HF_MODEL_ID, \n",
    "        local_dir=str(model_tar_dir), \n",
    "        local_dir_use_symlinks=False,\n",
    "        allow_patterns=allow_patterns,\n",
    "        cache_dir=\"/home/ec2-user/SageMaker/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39425f35-ece7-4888-9366-f54283075fc8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Tokenize\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a492cfc-8402-4fb1-8fe3-3b27c64c5c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point.get(\"input\"),\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = prompter.generate_prompt(data_point[\"instruction\"], data_point.get(\"input\"))\n",
    "        tokenized_user_prompt = tokenize(user_prompt, add_eos_token=add_eos_token)\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [-100] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870f0e1-eab7-45db-bd59-3bd5c9c19394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = data['train'].shuffle()#.select(range(100))\n",
    "lm_dataset = dataset.map(generate_and_tokenize_prompt)\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb5742-ded5-4914-8c64-ad13e06dd548",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Save dataset to S3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1328ad7-c912-4da5-a905-2859e2276cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "bucket = None\n",
    "if bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket)\n",
    "\n",
    "print(f\"SageMaker role arn: {role}\")\n",
    "print(f\"SageMaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"SageMaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1620deb3-6020-4a56-838e-6bf56dd50e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket_prefix = 'ko-llms/peft'\n",
    "dataset_prefix = 'alpaca-train'\n",
    "s3_data_path = f\"s3://{bucket}/{bucket_prefix}/{model_name}/dataset/{dataset_prefix}\"\n",
    "s3_pretrained_model_path = f\"s3://{bucket}/{bucket_prefix}/huggingface-models/{model_name}/\"\n",
    "print(f\"S3 data path: \\n {s3_data_path}\")\n",
    "print(f\"S3 pretrained model path: \\n {s3_pretrained_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c198f-c0a5-40f4-85fe-74c286903cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_debug_samples = 50\n",
    "lm_dataset.save_to_disk(s3_data_path)\n",
    "lm_dataset.select(range(num_debug_samples)).save_to_disk(dataset_prefix)\n",
    "print(f\"Number of samples for debugging: {num_debug_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9b5ce-de81-4218-963f-7c9fa086a5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store bucket_prefix dataset_prefix s3_data_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
