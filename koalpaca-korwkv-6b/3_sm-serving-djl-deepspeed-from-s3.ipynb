{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2662eb8f-ba8b-4c34-8ab9-b312ecfcff91",
   "metadata": {},
   "source": [
    "# Korean LLM (Large Language Model) Serving on SageMaker with AWS Large Model Container DLC\n",
    "---\n",
    "\n",
    "### Model: [KoAlpaca-KoRWKV-6B](https://huggingface.co/beomi/KoAlpaca-KoRWKV-6B)\n",
    "\n",
    "한국어 LLM 모델 SageMaker 서빙 핸즈온 (LLM을 모델을 S3에 복사 후 S3에서 s5cmd로 배포)\n",
    "\n",
    "- LLM GitHub: https://github.com/Beomi/KoAlpaca\n",
    "- [AWS Blog: Deploy large models on Amazon SageMaker using DJLServing and DeepSpeed model parallel inference](https://aws.amazon.com/ko/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a056d19-3339-4778-b73d-f5fe14d50ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd775a9f-ca09-4788-aaf6-840f3651d8f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU boto3 huggingface_hub sagemaker langchain deepspeed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca4169-bc8f-4eeb-b897-b48b15f1c6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker, boto3, jinja2\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\")  # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "jinja_env = jinja2.Environment()  # jinja environment to generate model configuration templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85236aea-d0e6-4b5f-89bf-2d185881740c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1. Download LLM model and upload it to S3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08780048-cc8a-4a3d-b336-807263746fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "model_id = \"beomi/KoAlpaca-KoRWKV-6B\"\n",
    "model_prefix = model_id.split('/')[-1].replace('.', '-')\n",
    "model_tar_dir = f\"/home/ec2-user/SageMaker/models/{model_prefix}\"\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.txt\", \"*.model\", \"*.safetensors\"]\n",
    "#allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\"]\n",
    "\n",
    "if not os.path.isdir(model_tar_dir):\n",
    "    os.makedirs(model_tar_dir, exist_ok=True)\n",
    "    # - Leverage the snapshot library to donload the model since the model is stored in repository using LFS    \n",
    "    snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        local_dir=str(model_tar_dir), \n",
    "        local_dir_use_symlinks=False,        \n",
    "        allow_patterns=allow_patterns,\n",
    "        cache_dir=\"/home/ec2-user/SageMaker/\"        \n",
    "    )\n",
    "\n",
    "bucket_prefix = 'ko-llms/serving'    \n",
    "s3_code_prefix = f\"{bucket_prefix}/{model_prefix}/code\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = f\"{bucket_prefix}/{model_prefix}/model\"  # folder where model checkpoint will go\n",
    "s3_model_artifact = f\"s3://{bucket}/{s3_model_prefix}\"\n",
    "\n",
    "print(f\"S3 code prefix \\n {s3_code_prefix}\")\n",
    "print(f\"S3 model prefix: \\n {s3_model_prefix}\")\n",
    "print(f\"S3 model artifact path: \\n {s3_model_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326a83c-348d-4b99-8258-b87f24316011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws configure set default.s3.max_concurrent_requests 100\n",
    "aws configure set default.s3.max_queue_size 10000\n",
    "aws configure set default.s3.multipart_threshold 1GB\n",
    "aws configure set default.s3.multipart_chunksize 64MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b84012c-0777-47ee-a88c-1d98b6fbd938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 sync {model_tar_dir} {s3_model_artifact}\n",
    "print(f\"Model uploaded to --- > {s3_model_artifact}\")\n",
    "print(f\"We will set option.s3url={s3_model_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fca204-ff77-49c5-bc76-b5383c8ded4e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Model Serving Scripts\n",
    "---\n",
    "### Create `serving.properties`\n",
    "\n",
    "이 설정 파일은 어떤 추론 최적화 라이브러리를 사용할지, 어떤 설정을 사용할지 DJL Serving에 알려주는 설정 파일입니다. 필요에 따라 적절한 구성을 설정할 수 있습니다.\n",
    "\n",
    "모델이 레이어에 따라 분할되는 파이프라인 병렬화(Pipeline Parallelism)를 사용하는 허깅페이스 Accelerate와 달리, DeepSpeed는 각 레이어(텐서)가 여러 디바이스에 걸쳐 샤딩되는 텐서 병렬화(Tensor Parallelism)를 사용합니다. 파이프라인 병렬 처리 접근 방식에서는 데이터가 각 GPU 장치를 통해 순차적으로 흐르지만, 텐서 병렬 처리는 데이터가 모든 GPU 장치로 전송되어 각 GPU에서 부분적인 결과가 계산됩니다. 그런 다음 All-Gather 연산을 통해 부분 결과를 수집하여 최종 결과를 계산합니다. 따라서, 텐서 병렬화가 일반적으로 더 높은 GPU 활용률과 더 나은 성능을 제공합니다.\n",
    "\n",
    "- `option.s3url` - 모델 파일의 위치를 지정합니다. 또는`option.model_id` 옵션을 대신 사용하여 허깅페이스 허브에서 모델을 지정할 수 있습니다(예: EleutherAI/gpt-j-6B). 그러면 허브에서 모델이 자동으로 다운로드됩니다. s3url 접근 방식은 자체 환경 내에서 모델 아티팩트를 호스팅할 수 있고 DJL 추론 컨테이너 내에서 최적화된 접근 방식을 활용하여 S3에서 호스팅 인스턴스로 모델을 전송함으로써 더 빠른 모델 배포가 가능합니다.\n",
    "\n",
    "`serving.properties`의 일반적인 설정법과 자세한 내용은 https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html 를 참조하세요.\n",
    "\n",
    "<img src=\"../images/TensorShard.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e960108-f6bf-4b8d-9ea6-78b6ca3c6915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_path = f\"src/{model_prefix}\"\n",
    "!rm -rf {src_path}\n",
    "os.makedirs(src_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beab7e1-205c-47b1-b27a-0120614caec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {src_path}/serving.properties\n",
    "option.s3url={{s3url}}\n",
    "\n",
    "engine=DeepSpeed\n",
    "\n",
    "# passing extra options to model.py or built-in handler\n",
    "job_queue_size=100\n",
    "batch_size=1\n",
    "max_batch_delay=1\n",
    "max_idle_time=60\n",
    "\n",
    "# Built-in entrypoint\n",
    "#option.entryPoint=djl_python.deepspeed\n",
    "\n",
    "# Hugging Face model id\n",
    "#option.model_id=EleutherAI/gpt-j-6B\n",
    "\n",
    "# defines custom environment variables\n",
    "#env=SERVING_NUMBER_OF_NETTY_THREADS=2\n",
    "\n",
    "# Allows to load DeepSpeed workers in parallel\n",
    "option.parallel_loading=true\n",
    "\n",
    "# specify tensor parallel degree (number of partitions)\n",
    "option.tensor_parallel_degree=4\n",
    "\n",
    "# specify per model timeout\n",
    "option.model_loading_timeout=600\n",
    "#option.predict_timeout=240\n",
    "\n",
    "# mark the model as failure after python process crashing 10 times\n",
    "retry_threshold=0\n",
    "\n",
    "option.task=text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8cacee-600c-460b-a0b1-7c263a26b5c7",
   "metadata": {},
   "source": [
    "### serving.properties의 S3 경로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f03823-aa2a-48e8-b862-116917c1cc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(f\"{src_path}/serving.properties\").open().read())\n",
    "Path(f\"{src_path}/serving.properties\").open(\"w\").write(template.render(s3url=s3_model_artifact))\n",
    "!pygmentize {src_path}/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76217c91-0a17-4895-a400-e8bd9889f00d",
   "metadata": {},
   "source": [
    "### Create model.py with custom inference code\n",
    "빌트인 추론 코드로 no-code로 배포할 수도 있지만, 커스텀 추론 코드를 작성하는 것도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c01fd-d6cc-4dfc-9dfe-8a63ebd8cf60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {src_path}/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "import logging\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GPTNeoXLayer\n",
    "\n",
    "predictor = None\n",
    "\n",
    "def get_model(properties):\n",
    "    \n",
    "    tp_degree = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties[\"model_dir\"]\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties[\"model_id\"]\n",
    "    task = properties[\"task\"]\n",
    "    \n",
    "    logging.info(f\"Loading model in {model_location}\")    \n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_location,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "    \n",
    "    ds_config = {\n",
    "        \"tensor_parallel\": {\"tp_size\": tp_degree},\n",
    "        \"dtype\": model.dtype,\n",
    "        \"injection_policy\": {\n",
    "            GPTNeoXLayer:('attention.dense', 'mlp.dense_4h_to_h')\n",
    "        }\n",
    "    }\n",
    "    logging.info(f\"Starting DeepSpeed init with TP={tp_degree}\")        \n",
    "    model = deepspeed.init_inference(model, ds_config)  \n",
    "    \n",
    "    generator = pipeline(\n",
    "        task=task, model=model, tokenizer=tokenizer, device=local_rank\n",
    "    )\n",
    "    # https://huggingface.co/docs/hub/models-tasks\n",
    "    return generator\n",
    "    \n",
    "def handle(inputs: Input) -> None:\n",
    "    \"\"\"\n",
    "    inputs: Contains the configurations from serving.properties\n",
    "    \"\"\"    \n",
    "    global predictor\n",
    "    if not predictor:\n",
    "        predictor = get_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        logging.info(\"is_empty\")\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_json() #inputs.get_as_string()\n",
    "    logging.info(\"data:\", data)\n",
    "    \n",
    "    input_prompt, params = data[\"inputs\"], data[\"parameters\"]\n",
    "    result = predictor(input_prompt, **params)\n",
    "    logging.info(\"result:\", result)\n",
    "\n",
    "    return Output().add_as_json(result) #Output().add(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a063b634-3a4f-49ec-8779-fdda2818326e",
   "metadata": {},
   "source": [
    "### Create the Tarball and then upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa1063-839a-46be-9ca4-f6b9c14efbc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf model.tar.gz\n",
    "!tar czvf model.tar.gz -C {src_path} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d60fd8a-8a0e-497c-8628-98eb26772cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")\n",
    "!rm -rf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea2358-3ba1-4ff3-9ca4-df772b59770d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Serve LLM Model on SageMaker\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820b1c3-7854-433d-bbb6-03193abefa22",
   "metadata": {},
   "source": [
    "### Create SageMaker Model\n",
    "\n",
    "SageMaker 엔드포인트 생성 매개변수 VolumeSizeInGB를 지정할 때 마운트되는 Amazon EBS(Amazon Elastic Block Store) 볼륨에 /tmp를 매핑하기 때문에 컨테이너는 인스턴스의 `/tmp` 공간에 모델을 다운로드합니다. 이때 s5cmd (https://github.com/peak/s5cmd) 를 활용하므로 대용량 모델을 빠르게 다운로드할 수 있습니다.\n",
    "볼륨 인스턴스와 함께 미리 빌드되어 제공되는 p4dn과 같은 인스턴스의 경우 컨테이너의 `/tmp`를 계속 활용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b6ded-ba9c-4f25-b862-090546607b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import image_uris\n",
    "\n",
    "img_uri = image_uris.retrieve(framework=\"djl-deepspeed\", region=region, version=\"0.23.0\")\n",
    "model_name = name_from_base(f\"{model_prefix}\")\n",
    "print(model_name)\n",
    "\n",
    "model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": img_uri, \"ModelDataUrl\": s3_code_artifact},\n",
    ")\n",
    "model_arn = model_response[\"ModelArn\"]\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96783b7-9e6a-4bed-8ff9-c779d9e628e4",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f670c-57f4-4092-af29-b1416829e9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "variant_name = \"variant1\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "initial_instance_count = 1\n",
    "\n",
    "prod_variants = [\n",
    "    {\n",
    "        \"VariantName\": \"variant1\",\n",
    "        \"ModelName\": model_name,\n",
    "        \"InstanceType\": instance_type,\n",
    "        \"InitialInstanceCount\": initial_instance_count,\n",
    "        # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "        \"ContainerStartupHealthCheckTimeoutInSeconds\": 600,\n",
    "    }\n",
    "]\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=prod_variants\n",
    ")\n",
    "\n",
    "endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf78f45-b06e-431c-9048-3ade776cac07",
   "metadata": {},
   "source": [
    "엔드포인트가 생성되는 동안 아래의 문서를 같이 확인해 보세요.\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6122a3f8-78b6-42b9-b390-af8942d8e30c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "def make_console_link(region, endpoint_name, task='[SageMaker LLM Serving]'):\n",
    "    endpoint_link = f'<b> {task} <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\">Check Endpoint Status</a></b>'   \n",
    "    return endpoint_link\n",
    "\n",
    "endpoint_link = make_console_link(region, endpoint_name)\n",
    "display(HTML(endpoint_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d43b855-4d79-4460-aecc-6128356214da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from inference_lib import describe_endpoint, Prompter\n",
    "describe_endpoint(endpoint_name)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b26c2e-b620-4df9-b712-c5aeb8e9e32a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd435259-7952-4a76-b4ee-3360da5dd7c5",
   "metadata": {},
   "source": [
    "엔드포인트를 호출할 때 이 텍스트를 JSON 페이로드 내에 제공해야 합니다. 이 JSON 페이로드에는 length, sampling strategy, output token sequence restrictions을 제어하는 데 도움이 되는 원하는 추론 매개변수가 포함될 수 있습니다. 허깅페이스 트랜스포머 transformers 라이브러리에는 [사용 가능한 페이로드 매개변수](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig)의 전체 목록이 정의되어 있지만, 중요한 페이로드 매개변수는 다음과 같이 정의되어 있습니다:\n",
    "\n",
    "* **do_sample (`bool`)** – logits sampling 활성화\n",
    "* **max_new_tokens (`int`)** – 생성된 토큰의 최대 수\n",
    "* **best_of (`int`)** – best_of 개의 시퀀스를 생성하고 가장 높은 토큰 로그 확률이 있는 경우 반환\n",
    "* **repetition_penalty (`float`)** – 반복 패널티에 대한 파라미터, 1.0은 패널티가 없음을 의미하여 Greedy 서치와 동일, 커질수록 다양한 결과를 얻을 수 있으며, 자세한 사항은 [this paper](https://arxiv.org/pdf/1909.05858.pdf)을 참고\n",
    "* **return_full_text (`bool`)** – 생성된 텍스트에 프롬프트를 추가할지 여부\n",
    "* **seed (`int`)** – Random sampling seed\n",
    "* **stop_sequences (`List[str]`)** – `stop_sequences` 가 생성되면 토큰 생성을 중지\n",
    "* **temperature (`float`)** – logits 분포 모듈화에 사용되는 값\n",
    "* **top_k (`int`)** – 상위 K개 만큼 가장 높은 확률 어휘 토큰의 수\n",
    "* **top_p (`float`)** – 1 보다 작게 설정하게 되며, 상위부터 정렬했을 때 가능한 토큰들의 확률을 합산하여 `top_p` 이상의 가장 작은 집합을 유지\n",
    "* **truncate (`int`)** – 입력 토큰을 지정된 크기로 잘라냄\n",
    "* **typical_p (`float`)** – typical Decoding 양으로, 자세한 사항은 [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666)을 참고\n",
    "* **watermark (`bool`)** –  [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)가 Watermarking\n",
    "* **decoder_input_details (`bool`)** – decoder input token logprobs와 ids를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12185205-fc49-4716-aacd-c1017a8541a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"temperature\": 0.4,\n",
    "    \"top_p\": 0.9,\n",
    "    \"return_full_text\": False,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"presence_penalty\": None,\n",
    "    \"eos_token_id\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de6f75-4066-44b8-bf4c-e22d34406c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from inference_lib import KoLLMSageMakerEndpoint, Prompter\n",
    "ep = KoLLMSageMakerEndpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4579f-7ef7-40d5-a943-ae8dc137b8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"다음 글을 알기 쉽게 요약해 주세요.\"\n",
    "context = \"\"\"\n",
    "대규모 언어 모델(LLM; Large Language Models) 분야의 발전과 LLM이 가치 있는 인사이트를 제공하는 문제 세트 수가 계속 증가하고 있다는 소식을 들어보셨을 겁니다. \n",
    "대규모 데이터 세트와 여러 작업을 통해 훈련된 대규모 모델은 훈련되지 않은 특정 작업에도 잘 일반화됩니다. 이러한 모델을 파운데이션 모델(foundation model)이라고 하며, 스탠포드 HAI 연구소(Stanford Institute for Human-Centered Artificial Intelligence)에서 처음 대중화한 용어입니다. \n",
    "이러한 파운데이션 모델은 프롬프트 엔지니어링(prompt engineering) 기법의 도움으로 잘 활용할 수 있지만, 유스케이스가 도메인에 매우 특화되어 있거나 작업의 종류가 매우 다양하여 모델을 추가로 커스터마이징해야 하는 경우가 많습니다. \n",
    "특정 도메인이나 작업에 대한 대규모 모델의 성능을 개선하기 위한 한 가지 접근 방식은 더 작은 작업별 데이터 세트로 모델을 추가로 훈련하는 것입니다. 파인 튜닝(fine-tuning)으로 알려진 이 접근 방식은 LLM의 정확도를 성공적으로 개선하지만, 모든 모델 가중치를 수정해야 합니다. \n",
    "파인 튜닝 데이터 세트 크기가 훨씬 작기 때문에 사전 훈련(pre-training)하는 것 보다 훨씬 빠르지만 여전히 상당한 컴퓨팅 성능과 메모리가 필요합니다. \n",
    "파인 튜닝은 원본 모델의 모든 파라미터 가중치를 수정하므로 비용이 많이 들고 원본 모델과 동일한 크기의 모델을 생성하므로 스토리지 용량에도 부담이 됩니다.\n",
    "\"\"\"\n",
    "payload = ep.get_payload(instruction, context, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b13345-ff32-4ad9-9953-f90d2ac7359f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_text = ep.infer(payload, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed0e37-20af-467f-804b-7cbb7cbf8101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "고객님: 안녕하세요, iPhone에 문제가 있습니다.\n",
    "상담원: 안녕하세요! 무슨 문제인가요?\n",
    "고객님: 휴대폰이 제대로 충전되지 않고 배터리가 매우 빨리 소모되는 것 같습니다. 다른 충전 케이블과 전원 어댑터를 사용해 보았지만 문제가 지속됩니다.\n",
    "상담원: 흠, 그렇군요. 몇 가지 문제 해결 단계를 시도해 보겠습니다. 설정, 배터리로 이동하여 배터리 수명을 많이 소모하는 앱이 있는지 확인해 주시겠어요?\n",
    "고객님: 예, 배터리를 많이 소모하는 앱이 몇 개 있습니다.\n",
    "상담원: 좋아요, 화면 하단에서 위로 스와이프하여 해당 앱을 강제 종료한 다음 앱을 위로 스와이프하여 닫아 보세요.\n",
    "고객: 그렇게 했는데도 문제가 여전히 남아 있습니다.\n",
    "상담원: 네, iPhone의 설정을 기본값으로 재설정해 보겠습니다. 이렇게 해도 데이터가 삭제되지는 않습니다. 설정, 일반, 재설정으로 이동한 다음 모든 설정 재설정을 선택하세요.\n",
    "고객님: 그렇게 했습니다. 다음은 어떻게 해야 하나요?\n",
    "상담원: 이제 iPhone을 재시동해 보겠습니다. \"밀어서 전원 끄기\" 옵션이 표시될 때까지 전원 버튼을 길게 누릅니다. 밀어 전원을 끄고 몇 초간 기다린 다음 iPhone을 다시 켜세요.\n",
    "고객님: 다시 시작했지만 여전히 제대로 충전되지 않습니다.\n",
    "상담원: 그렇군요. iPhone에서 진단 테스트를 실행해야 할 것 같습니다. 가까운 Apple Store 또는 공인 서비스 제공업체를 방문하여 iPhone을 점검받으시기 바랍니다.\n",
    "고객: 예약을 해야 하나요?\n",
    "상담원: 예. 줄을 서서 기다리지 않으려면 항상 미리 예약하는 것이 가장 좋습니다. 온라인으로 예약하거나 Apple Store 또는 공인 서비스 제공업체에 전화하여 예약할 수 있습니다.\n",
    "고객님: 수리 비용은 제가 지불해야 하나요?\n",
    "상담원: iPhone에 보증이 적용되는지 여부에 따라 다릅니다. 보증이 적용되는 경우에는 비용을 지불할 필요가 없습니다. 그러나 보증이 적용되지 않는 경우에는 수리 비용을 지불하셔야 합니다.\n",
    "고객님: iPhone을 돌려받는 데 얼마나 걸리나요?\n",
    "상담원: 문제의 심각도에 따라 다르지만 일반적으로 영업일 기준 1~2일이 소요됩니다.\n",
    "고객: 온라인으로 수리 상태를 추적할 수 있나요?\n",
    "상담원: 온라인 또는 Apple Store 또는 공인 서비스 제공업체에 전화하여 수리 상태를 추적할 수 있습니다.\n",
    "고객: 알겠습니다. 도와주셔서 감사합니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632df1b8-5bdb-4365-b84d-768a68951638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "instruction = 'iPhone 충전 문제를 해결하기 위해 고객에게 어떤 문제 해결 단계를 제안하나요?'\n",
    "payload = ep.get_payload(instruction, input_text, params)\n",
    "generated_text = ep.infer(payload, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b015c22-caab-4fda-907f-e7187f83a4b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "instruction = '고객이 iPhone 수리를 위해 가까운 Apple Store 또는 공인 서비스 제공업체에 예약하려면 어떤 조치를 취해야 하나요?'\n",
    "payload = ep.get_payload(instruction, input_text, params)\n",
    "generated_text = ep.infer(payload, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6bcdb9-b73d-468f-8606-6bf4b2f90a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625410bd-e2e9-4d57-bb35-bddf5cf20301",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. Clean Up\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972317bf-03e9-424b-827c-195c858fb95f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44680c92-0623-46a0-9d3a-2efa262f9af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Delete the end point\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# - In case the end point failed we still want to delete the model\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feeb821-db0a-4a48-8550-b0146705b8d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "# References\n",
    "---\n",
    "\n",
    "- Model 정보\n",
    "    - kullm-polyglot-5.8b-v2\n",
    "        - This model is a parameter-efficient fine-tuned version of EleutherAI/polyglot-ko-5.8b on a KULLM v2\n",
    "        - https://huggingface.co/nlpai-lab/kullm-polyglot-5.8b-v2        \n",
    "    - kullm-polyglot-12.8b-v2\n",
    "        - This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KULLM v2\n",
    "        - https://huggingface.co/nlpai-lab/kullm-polyglot-12.8b-v2\n",
    "    - beomi/KoAlpaca-Polyglot-12.8B\n",
    "        - This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KoAlpaca Dataset v1.1b\n",
    "        - https://huggingface.co/beomi/KoAlpaca-Polyglot-12.8B\n",
    "    - EleutherAI/polyglot-ko-12.8b\n",
    "        - Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token.\n",
    "        - License: Apache 2.0\n",
    "        - https://huggingface.co/EleutherAI/polyglot-ko-12.8b      \n",
    "- 코드\n",
    "    - [Boto3](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/pytorch_deploy_large_GPT_model/GPT-J-6B-model-parallel-inference-DJL.ipynb)\n",
    "    - [Python SDK](https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/deepspeed/GPT-J-6B_DJLServing_with_PySDK.ipynb)\n",
    "    - [Kor LLM on SageMaker](https://github.com/gonsoomoon-ml/Kor-LLM-On-SageMaker)\n",
    "    - [AWS Generative AI Workshop for Korean language](https://github.com/aws-samples/aws-ai-ml-workshop-kr/tree/master/genai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
