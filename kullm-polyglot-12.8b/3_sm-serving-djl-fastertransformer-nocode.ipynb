{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2662eb8f-ba8b-4c34-8ab9-b312ecfcff91",
   "metadata": {},
   "source": [
    "# Korean LLM (Large Language Model) Serving on SageMaker with AWS Large Model Container DLC\n",
    "---\n",
    "\n",
    "한국어 LLM 모델 SageMaker 서빙 핸즈온 (No inference code)\n",
    "\n",
    "- LLM GitHub: https://github.com/nlpai-lab/KULLM\n",
    "- Hugging Face model hub: https://huggingface.co/nlpai-lab/kullm-polyglot-5.8b-v2\n",
    "- [AWS Blog: Deploy large models on Amazon SageMaker using DJLServing and DeepSpeed model parallel inference](https://aws.amazon.com/ko/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a056d19-3339-4778-b73d-f5fe14d50ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b73dc3e-82d3-4e08-983f-75ec6c83c6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU boto3 huggingface_hub sagemaker langchain deepspeed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ca4169-bc8f-4eeb-b897-b48b15f1c6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker, boto3, jinja2\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\")  # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "jinja_env = jinja2.Environment()  # jinja environment to generate model configuration templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85236aea-d0e6-4b5f-89bf-2d185881740c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1. Download LLM model and upload it to S3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a104756-a389-403c-bb3a-b512ed6b948d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002870498cf84a158c73b2f2e959a88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\"/home/ec2-user/SageMaker/hf_cache\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_id = \"nlpai-lab/kullm-polyglot-12.8b-v2\"\n",
    "model_prefix = model_id.split('/')[-1].replace('.', '-')\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_id,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")\n",
    "\n",
    "s3_code_prefix = f\"ko-llm/{model_prefix}/code\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = f\"ko-llm/{model_prefix}/model\"  # folder where model checkpoint will go\n",
    "s3_model_artifact = f\"s3://{bucket}/{s3_model_prefix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c326a83c-348d-4b99-8258-b87f24316011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws configure set default.s3.max_concurrent_requests 100\n",
    "aws configure set default.s3.max_queue_size 10000\n",
    "aws configure set default.s3.multipart_threshold 1GB\n",
    "aws configure set default.s3.multipart_chunksize 64MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b84012c-0777-47ee-a88c-1d98b6fbd938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to --- > s3://sagemaker-us-east-1-143656149352/ko-llm/kullm-polyglot-12-8b-v2/model\n",
      "We will set option.s3url=s3://sagemaker-us-east-1-143656149352/ko-llm/kullm-polyglot-12-8b-v2/model\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync {model_download_path} {s3_model_artifact}\n",
    "print(f\"Model uploaded to --- > {s3_model_artifact}\")\n",
    "print(f\"We will set option.s3url={s3_model_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fca204-ff77-49c5-bc76-b5383c8ded4e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Model Serving Scripts\n",
    "---\n",
    "### Create `serving.properties`\n",
    "\n",
    "이 설정 파일은 어떤 추론 최적화 라이브러리를 사용할지, 어떤 설정을 사용할지 DJL Serving에 알려주는 설정 파일입니다. 필요에 따라 적절한 구성을 설정할 수 있습니다.\n",
    "\n",
    "모델이 레이어에 따라 분할되는 파이프라인 병렬화(Pipeline Parallelism)를 사용하는 허깅페이스 Accelerate와 달리, DeepSpeed는 각 레이어(텐서)가 여러 디바이스에 걸쳐 샤딩되는 텐서 병렬화(Tensor Parallelism)를 사용합니다. 파이프라인 병렬 처리 접근 방식에서는 데이터가 각 GPU 장치를 통해 순차적으로 흐르지만, 텐서 병렬 처리는 데이터가 모든 GPU 장치로 전송되어 각 GPU에서 부분적인 결과가 계산됩니다. 그런 다음 All-Gather 연산을 통해 부분 결과를 수집하여 최종 결과를 계산합니다. 따라서, 텐서 병렬화가 일반적으로 더 높은 GPU 활용률과 더 나은 성능을 제공합니다.\n",
    "\n",
    "- `option.s3url` - 모델 파일의 위치를 지정합니다. 또는`option.model_id` 옵션을 대신 사용하여 허깅페이스 허브에서 모델을 지정할 수 있습니다(예: EleutherAI/gpt-j-6B). 그러면 허브에서 모델이 자동으로 다운로드됩니다. s3url 접근 방식은 자체 환경 내에서 모델 아티팩트를 호스팅할 수 있고 DJL 추론 컨테이너 내에서 최적화된 접근 방식을 활용하여 S3에서 호스팅 인스턴스로 모델을 전송함으로써 더 빠른 모델 배포가 가능합니다.\n",
    "\n",
    "`serving.properties`의 일반적인 설정법과 자세한 내용은 https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html 를 참조하세요.\n",
    "\n",
    "<img src=\"../images/TensorShard.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e960108-f6bf-4b8d-9ea6-78b6ca3c6915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_path = f\"src/{model_prefix}\"\n",
    "!rm -rf {src_path}\n",
    "os.makedirs(src_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5beab7e1-205c-47b1-b27a-0120614caec2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/kullm-polyglot-12-8b-v2/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile {src_path}/serving.properties\n",
    "engine = FasterTransformer\n",
    "option.entryPoint = djl_python.fastertransformer\n",
    "option.s3url = {{s3url}}\n",
    "option.tensor_parallel_degree = 4\n",
    "option.dtype = fp16\n",
    "option.task = text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8cacee-600c-460b-a0b1-7c263a26b5c7",
   "metadata": {},
   "source": [
    "### serving.properties의 S3 경로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34f03823-aa2a-48e8-b862-116917c1cc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001b[36mengine\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mFasterTransformer\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     2\t\u001b[36moption.entryPoint\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mdjl_python.fastertransformer\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     3\t\u001b[36moption.s3url\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33ms3://sagemaker-us-east-1-143656149352/ko-llm/kullm-polyglot-12-8b-v2/model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     4\t\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m4\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     5\t\u001b[36moption.dtype\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mfp16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     6\t\u001b[36moption.task\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mtext-generation\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(f\"{src_path}/serving.properties\").open().read())\n",
    "Path(f\"{src_path}/serving.properties\").open(\"w\").write(template.render(s3url=s3_model_artifact))\n",
    "!pygmentize {src_path}/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a063b634-3a4f-49ec-8779-fdda2818326e",
   "metadata": {},
   "source": [
    "### Create the Tarball and then upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90fa1063-839a-46be-9ca4-f6b9c14efbc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm -rf model.tar.gz\n",
    "!tar czvf model.tar.gz -C {src_path} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d60fd8a-8a0e-497c-8628-98eb26772cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-143656149352/ko-llm/kullm-polyglot-12-8b-v2/code/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")\n",
    "#!rm -rf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea2358-3ba1-4ff3-9ca4-df772b59770d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Serve LLM Model on SageMaker\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820b1c3-7854-433d-bbb6-03193abefa22",
   "metadata": {},
   "source": [
    "### Create SageMaker Model\n",
    "\n",
    "SageMaker 엔드포인트 생성 매개변수 VolumeSizeInGB를 지정할 때 마운트되는 Amazon EBS(Amazon Elastic Block Store) 볼륨에 /tmp를 매핑하기 때문에 컨테이너는 인스턴스의 `/tmp` 공간에 모델을 다운로드합니다. 이때 s5cmd (https://github.com/peak/s5cmd) 를 활용하므로 대용량 모델을 빠르게 다운로드할 수 있습니다.\n",
    "볼륨 인스턴스와 함께 미리 빌드되어 제공되는 p4dn과 같은 인스턴스의 경우 컨테이너의 `/tmp`를 계속 활용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "598b6ded-ba9c-4f25-b862-090546607b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.23.0-fastertransformer5.3.0-cu118\n",
      "kullm-polyglot-12-8b-v2-2023-07-24-13-41-56-921\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import image_uris\n",
    "\n",
    "img_uri = image_uris.retrieve(framework=\"djl-fastertransformer\", region=region, version=\"0.23.0\")\n",
    "model_name = name_from_base(f\"{model_prefix}\")\n",
    "print(f\"Image going to be used is ---- > {img_uri}\")\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b94cef46-9058-4c8e-8006-17cc8a5cb553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-143656149352/ko-llm/kullm-polyglot-12-8b-v2/code/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")\n",
    "env = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\", \"TRANSFORMERS_CACHE\": \"/tmp\"}\n",
    "\n",
    "model = Model(image_uri=img_uri, model_data=s3_code_artifact, env=env, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96783b7-9e6a-4bed-8ff9-c779d9e628e4",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3941cd-4ade-4e8e-8d33-6becb07fa82d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kullm-polyglot-12-8b-v2-djl-ft-2023-07-24-13-42-21-716\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "endpoint_name = name_from_base(f\"{model_prefix}-djl-ft\")\n",
    "print(endpoint_name)\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=3600,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf78f45-b06e-431c-9048-3ade776cac07",
   "metadata": {},
   "source": [
    "엔드포인트가 생성되는 동안 아래의 문서를 같이 확인해 보세요.\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6122a3f8-78b6-42b9-b390-af8942d8e30c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b> [SageMaker LLM Serving] <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints/kullm-polyglot-12-8b-v2-djl-ft-2023-07-24-13-42-21-716\">Check Endpoint Status</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "def make_console_link(region, endpoint_name, task='[SageMaker LLM Serving]'):\n",
    "    endpoint_link = f'<b> {task} <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\">Check Endpoint Status</a></b>'   \n",
    "    return endpoint_link\n",
    "\n",
    "endpoint_link = make_console_link(region, endpoint_name)\n",
    "display(HTML(endpoint_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc30ad-d2a0-47ea-92cb-3d99d784fe53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from inference_lib import describe_endpoint, Prompter\n",
    "describe_endpoint(endpoint_name)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b26c2e-b620-4df9-b712-c5aeb8e9e32a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44db971e-ecc1-402c-b813-edbfab82724a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c0593cc-181a-471e-8945-dc627ece22d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 ms, sys: 4.2 ms, total: 17.9 ms\n",
      "Wall time: 3.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '!A: 아마존 웹 서비스(AWS)는 클라우드 컴퓨팅 서비스를 제공하는 회사입니다. 클라우드 컴퓨팅은 인터넷을 통해 컴퓨팅 리소스를 제공하는 것을 말합니다. AWS는 컴퓨팅 리소스를 제공하고, 이를 통해 기업이 애플리케이션과 서비스를 구축하고 실행할 수 있도록 지원합니다.AWS는 전 세계에서 가장 큰 클라우드 컴퓨팅 서비스 제공업체 중 하나이며, 전 세계에서 가장 큰 클라우드 컴퓨팅 회사입니다.AWS는 고객이 AWS를 사용하여 애플리케이션과 서비스를 구축하고 실행할 수 있도록 지원하는 서비스를 제공합니다. AWS는 고객이 AWS를 사용하여 애플리케이션과 서비스를 구축하고 실행할 수 있도록 지원하는 서비스를 제공합니다.AWS는 고객이 AWS를 사용하여 애플리케이션과 서비스를 구축하고 실행할 수 있도록 지원하는 서비스를 제공합니다.AWS는 고객이 AWS를 사용하여 애플리케이션과 서비스를 구축하고 실행할'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predictor.predict(\n",
    "    {\n",
    "        \"inputs\": [\"아마존 웹 서비스에 대해서 자세히 알려줘\"],\n",
    "        \"parameters\": {\"max_seq_len\": 200, \"temperature\": 0.1},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97a9a12f-bdcc-498a-a132-9b99441c3662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from inference_lib import KoLLMSageMakerEndpoint\n",
    "\n",
    "ep = KoLLMSageMakerEndpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd52a8fe-42f8-46f4-af34-4999af60c983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_seq_len\": 512,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "instruction = \"\"\n",
    "input_text = \"아마존 웹 서비스에 대해서 자세히 알려줘\"\n",
    "payload = ep.get_payload(instruction, input_text, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cce18e21-c4c0-412c-a08d-8bc31c1baeb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.83 ms, sys: 0 ns, total: 2.83 ms\n",
      "Wall time: 8.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'AWS는 Amazon Web Services의 약자입니다. 클라우드 컴퓨팅 서비스를 제공하는 선도 기술 회사입니다. AWS는 전 세계 고객에게 인프라, 애플리케이션, 서비스를 제공하는 데 사용되는 클라우드 컴퓨팅 플랫폼입니다. AWS는 고객이 온 컴퓨팅을 통해 애플리케이션과 애플리케이션을 저장하고 확장할 수 있는 서비스형 인프라(IaaS) 및 서비스형 플랫폼(PaaS)을 제공합니다. 또한 AWS는 고객이 클라우드 기반 애플리케이션을 빌, 테스트 및 관리할 수 있는 서비스형 소프트웨어(SaaS)도 제공합니다. AWS는 고객이 클라우드 컴퓨팅을 최대한 활용할 수 있도록 지원하는 데 중점을 두고 있으며, 고객이 클라우드 기반 애플리케이션을 최대한 활용할 수 있도록 지원하는 데 중점을 두고 있습니다.<|endoftext|>[머니투데이 최광 기자][[the300]\"국민의당, \\'새정치민주연합\\'으로 당명 변경]]본문 이미지 영역 새정치민주연합 문재인 대표가 13일 오후 서울 여의도 국회에서 열린 당무위원회의에 참석해 생각에 잠겨 있다. 2015.12.21/뉴스1 문재인 새정치민주연합 대표는 21일 당무국민의당이 \\'새정치민주연합\\'으로 당명을 변경하는 것을 환영한다\"고 밝혔다.문 대표는 이날 국회에서 열린 당무위원회의에서 \"국민의당이 \\'새정치민주연합\\'으로 당명을 변경하는 것은 국민의당이 새정치민주연합에서 소중과 가치를 존중하고 계승하겠다는 의지를 천명한 것으로 평가한다\"며 이같이 말했다.문 대표는 \"국민의당이 새정치민주연합으로 당명을 변경하 것은 야권의 분열을 극복하고 통합의 이루겠다는 의지를 천명한 것으로 평가한다\"며 \"새의당이 새정치민주연합의 당명 변경을 존중하고 수용한 것을 환영한다\"고 덧붙였다.[내 삶을 바꾸는 정치뉴스 \\'the 300\\' 바로가기][\\'스페셜 걸\\' 포토][손안의 경제뉴스 머니투데이 모바일웹][\\'취업의 모든 것\\' 잡드림 ]최광기자 hollim324@mt.co.kr<|endoftext|>안녕\\'m or that he will be able to compose a comuntry if he wants to. It'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d6bcdb9-b73d-468f-8606-6bf4b2f90a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625410bd-e2e9-4d57-bb35-bddf5cf20301",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. Clean Up\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f69e5266-ac6d-4ae1-afab-815a8620c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cf1594c-7e7a-4ba5-9ba9-1cb8e1646573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feeb821-db0a-4a48-8550-b0146705b8d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "# References\n",
    "---\n",
    "\n",
    "- Model 정보\n",
    "    - kullm-polyglot-5.8b-v2\n",
    "        - This model is a parameter-efficient fine-tuned version of EleutherAI/polyglot-ko-5.8b on a KULLM v2\n",
    "        - https://huggingface.co/nlpai-lab/kullm-polyglot-5.8b-v2        \n",
    "    - kullm-polyglot-12.8b-v2\n",
    "        - This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KULLM v2\n",
    "        - https://huggingface.co/nlpai-lab/kullm-polyglot-12.8b-v2\n",
    "    - beomi/KoAlpaca-Polyglot-12.8B\n",
    "        - This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KoAlpaca Dataset v1.1b\n",
    "        - https://huggingface.co/beomi/KoAlpaca-Polyglot-12.8B\n",
    "    - EleutherAI/polyglot-ko-12.8b\n",
    "        - Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token.\n",
    "        - License: Apache 2.0\n",
    "        - https://huggingface.co/EleutherAI/polyglot-ko-12.8b      \n",
    "- 코드\n",
    "    - [Boto3](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/pytorch_deploy_large_GPT_model/GPT-J-6B-model-parallel-inference-DJL.ipynb)\n",
    "    - [Python SDK](https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/deepspeed/GPT-J-6B_DJLServing_with_PySDK.ipynb)\n",
    "    - [Kor LLM on SageMaker](https://github.com/gonsoomoon-ml/Kor-LLM-On-SageMaker)\n",
    "    - [AWS Generative AI Workshop for Korean language](https://github.com/aws-samples/aws-ai-ml-workshop-kr/tree/master/genai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
